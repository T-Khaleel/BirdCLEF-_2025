{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **BirdCLEF 2025 Data Preprocessing Notebook**\nThis notebook demonstrates how we can transform audio data into mel-spectrogram data. This transformation is essential for training 2D Convolutional Neural Networks (CNNs) on audio data, as it converts the one-dimensional audio signals into two-dimensional image-like representations.\nI run this public notebook in debug mode(only a few sample processing). You can find the fully preprocessed mel spectrogram training dataset here --> [BirdCLEF'25 | Mel Spectrograms](https://www.kaggle.com/datasets/kadircandrisolu/birdclef25-mel-spectrograms).\n","metadata":{}},{"cell_type":"code","source":"# Imports\nimport os\nimport cv2\nimport math\nimport time\nimport librosa\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport pandas as pd\n\nimport torch\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.220090Z","iopub.execute_input":"2025-05-15T12:42:25.220506Z","iopub.status.idle":"2025-05-15T12:42:25.226006Z","shell.execute_reply.started":"2025-05-15T12:42:25.220474Z","shell.execute_reply":"2025-05-15T12:42:25.224828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step1: Load the Silero model**","metadata":{}},{"cell_type":"code","source":"# Loading this requires internet on the first run, but cashes it locally\nmodel, utils = torch.hub.load(\n    repo_or_dir='snakers4/silero-vad',\n    model='silero_vad',\n    force_reload=False\n)\n(get_speech_timestamps, _, read_audio, _, _) = utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.227466Z","iopub.execute_input":"2025-05-15T12:42:25.227835Z","iopub.status.idle":"2025-05-15T12:42:25.393032Z","shell.execute_reply.started":"2025-05-15T12:42:25.227800Z","shell.execute_reply":"2025-05-15T12:42:25.391977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function for removing human voice\ndef remove_human_voice(audio: np.ndarray, sr: int) -> np.ndarray:\n    audio_t = torch.from_numpy(audio).float()\n    speech_ts = get_speech_timestamps(audio_t, model, sampling_rate=sr)\n    if not speech_ts:\n        return audio  # no speech detected\n\n    mask = torch.ones_like(audio_t)\n    for seg in speech_ts:\n        mask[seg['start']:seg['end']] = 0.0\n\n    return (audio_t * mask).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.395033Z","iopub.execute_input":"2025-05-15T12:42:25.395342Z","iopub.status.idle":"2025-05-15T12:42:25.401494Z","shell.execute_reply.started":"2025-05-15T12:42:25.395315Z","shell.execute_reply":"2025-05-15T12:42:25.400237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration function\nclass Config:\n    # Debug mode implemented to debug the models\n    DEBUG_MODE = False\n    # Identifying paths\n    OUTPUT_DIR = '/kaggle/working/'\n    DATA_ROOT = '/kaggle/input/birdclef-2025'\n\n    FS = 32000 # Hz\n    \n    # Mel spectrogram parameters\n    N_FFT = 1024 # Size of the FFT window\n    HOP_LENGTH = 512 # Number of audio samples between frames\n    N_MELS = 128 # Number of mel bands to generate\n    FMIN = 50 # Lowest frequency (Hz) included\n    FMAX = 14000 # Highest frequency (Hz) included\n    \n    TARGET_DURATION = 5.0 # Duration of audio clips (s)\n    TARGET_SHAPE = (256, 256)  # Target shape of the spectrogram\n    \n    N_MAX = 50 if DEBUG_MODE else None   # Number of audio files to process if in debug mode\n\n    # MFCC specific parameters\n    n_mfcc = 40 # Number of MFCCs to extract\n\n    # CQT specific parameters\n    n_bins=84  # Number of frequency bins for CQT\n    bins_per_octave=12 # Number of frequency bins per octave\n\nconfig = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.402911Z","iopub.execute_input":"2025-05-15T12:42:25.403297Z","iopub.status.idle":"2025-05-15T12:42:25.419577Z","shell.execute_reply.started":"2025-05-15T12:42:25.403256Z","shell.execute_reply":"2025-05-15T12:42:25.418443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary of debug mode enabled and number of samples\nprint(f\"Debug mode: {'ON' if config.DEBUG_MODE else 'OFF'}\")\nprint(f\"Max samples to process: {config.N_MAX if config.N_MAX is not None else 'ALL'}\")\n\n# Loading taxonomy and metadata\nprint(\"Loading taxonomy data...\")\ntaxonomy_df = pd.read_csv(f'{config.DATA_ROOT}/taxonomy.csv')\nspecies_class_map = dict(zip(taxonomy_df['primary_label'], taxonomy_df['class_name']))\n\nprint(\"Loading training metadata...\")\ntrain_df = pd.read_csv(f'{config.DATA_ROOT}/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.420724Z","iopub.execute_input":"2025-05-15T12:42:25.421066Z","iopub.status.idle":"2025-05-15T12:42:25.643411Z","shell.execute_reply.started":"2025-05-15T12:42:25.421039Z","shell.execute_reply":"2025-05-15T12:42:25.642496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Label extraction\nlabel_list = sorted(train_df['primary_label'].unique())\nlabel_id_list = list(range(len(label_list)))\nlabel2id = dict(zip(label_list, label_id_list))\nid2label = dict(zip(label_id_list, label_list))\n\n# Creation of working_df file\nprint(f'Found {len(label_list)} unique species')\nworking_df = train_df[['primary_label', 'rating', 'filename']].copy()\nworking_df['target'] = working_df.primary_label.map(label2id)\nworking_df['filepath'] = config.DATA_ROOT + '/train_audio/' + working_df.filename\nworking_df['samplename'] = working_df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\nworking_df['class'] = working_df.primary_label.map(lambda x: species_class_map.get(x, 'Unknown'))\ntotal_samples = min(len(working_df), config.N_MAX or len(working_df))\nprint(f'Total samples to process: {total_samples} out of {len(working_df)} available')\nprint(f'Samples by class:')\nprint(working_df['class'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.644413Z","iopub.execute_input":"2025-05-15T12:42:25.644781Z","iopub.status.idle":"2025-05-15T12:42:25.717114Z","shell.execute_reply.started":"2025-05-15T12:42:25.644746Z","shell.execute_reply":"2025-05-15T12:42:25.716137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to calculate the mel-spectrogram\ndef audio2melspec(audio_data):\n    # Replace not numbers with mean signal\n    if np.isnan(audio_data).any():\n        mean_signal = np.nanmean(audio_data)\n        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n\n    # Using the librosa library to calculate the mel-spectrogram \n    mel_spec = librosa.feature.melspectrogram(\n        y=audio_data,\n        sr=config.FS,\n        n_fft=config.N_FFT,\n        hop_length=config.HOP_LENGTH,\n        n_mels=config.N_MELS,\n        fmin=config.FMIN,\n        fmax=config.FMAX,\n        power=2.0\n    )\n\n    # Normalization to dB\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n    \n    return mel_spec_norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.718010Z","iopub.execute_input":"2025-05-15T12:42:25.718338Z","iopub.status.idle":"2025-05-15T12:42:25.724038Z","shell.execute_reply.started":"2025-05-15T12:42:25.718312Z","shell.execute_reply":"2025-05-15T12:42:25.722960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print statements to follow processing\nprint(\"Starting audio processing...\")\nprint(f\"{'DEBUG MODE - Processing only 50 samples' if config.DEBUG_MODE else 'FULL MODE - Processing all samples'}\")\n# Timer\nstart_time = time.time()\n\n# Storage\nall_bird_data = {}\nerrors = []\n\n# Fill the all_bird_data file with calculated feature representation\nfor i, row in tqdm(working_df.iterrows(), total=total_samples):\n    if config.N_MAX is not None and i >= config.N_MAX:\n        break\n    \n    try:\n        audio_data, _ = librosa.load(row.filepath, sr=config.FS)\n        if audio_data is None or audio_data.size == 0:\n            print(f\"Skipping empty file {row.filepath}\")\n            errors.append((row.filepath, \"Empty audio_data\"))\n            continue\n        audio_novoice = remove_human_voice(audio_data, config.FS)\n        \n        target_samples = int(config.TARGET_DURATION * config.FS)\n        if len(audio_novoice) < target_samples:\n            n_copy = math.ceil(target_samples / len(audio_novoice))\n            audio_novoice = np.tile(audio_novoice, n_copy)\n        start = len(audio_novoice)//2 - target_samples//2\n        clip = audio_novoice[max(0, start): max(0, start)+target_samples]\n        if len(clip) < target_samples:\n            clip = np.pad(clip, (0, target_samples-len(clip)))\n            \n        print(\"calculate mel_spec\")\n        mel_spec = audio2melspec(clip)\n\n        if mel_spec.shape != config.TARGET_SHAPE:\n            mel_spec = cv2.resize(mel_spec, config.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n        features = np.stack([mel_spec], axis=0).astype(np.float32)\n        all_bird_data[row.samplename] = features\n        \n    except Exception as e:\n        print(f\"Error processing {row.filepath}: {e}\")\n        errors.append((row.filepath, str(e)))\n\n# Print processing time and files that did and did not succeed to process\nend_time = time.time()\nprint(f\"Processing completed in {end_time - start_time:.2f} seconds\")\nprint(f\"Successfully processed {len(all_bird_data)} files out of {total_samples} total\")\nprint(f\"Failed to process {len(errors)} files\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:42:25.763476Z","iopub.execute_input":"2025-05-15T12:42:25.763824Z","iopub.status.idle":"2025-05-15T12:43:48.193504Z","shell.execute_reply.started":"2025-05-15T12:42:25.763793Z","shell.execute_reply":"2025-05-15T12:43:48.192363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"working_df['samplename']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:43:48.194556Z","iopub.execute_input":"2025-05-15T12:43:48.195103Z","iopub.status.idle":"2025-05-15T12:43:48.202816Z","shell.execute_reply.started":"2025-05-15T12:43:48.195059Z","shell.execute_reply":"2025-05-15T12:43:48.201722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the spectrogram\nimport matplotlib.pyplot as plt\n\nsamples = []\ndisplayed_classes = set()\n\nmax_samples = min(4, len(all_bird_data))\n\nfor i, row in working_df.iterrows():\n    if i >= (config.N_MAX or len(working_df)):\n        break\n        \n    if row['samplename'] in all_bird_data:\n        if row['class'] not in displayed_classes:\n            samples.append((row['samplename'], row['class'], row['primary_label']))\n            displayed_classes.add(row['class'])\n        if len(samples) >= max_samples:  \n            break\n\nif samples:\n    samplename, class_name, species = samples[0]\n    \n    feat = all_bird_data[samplename][0]   # shape: (3, H, W)\n    plt.figure(figsize=(6, 4))\n\n    plt.imshow(feat, aspect='auto', origin='lower', cmap='viridis')\n    plt.title(f\"Mel-Spec — {class_name}:{species}\")\n    plt.colorbar(fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:43:48.203835Z","iopub.execute_input":"2025-05-15T12:43:48.204104Z","iopub.status.idle":"2025-05-15T12:43:49.435450Z","shell.execute_reply.started":"2025-05-15T12:43:48.204081Z","shell.execute_reply":"2025-05-15T12:43:49.434231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize grid of spectrograms for the first few samples\n# Decide how many to show\nn_show = 10\n# Grab the first n_show sample names\nsample_names = list(all_bird_data.keys())[:n_show]\nfig, axes = plt.subplots(2, 5, figsize=(15, 6), constrained_layout=True)\nfig.suptitle(f'Top {n_show} samples — Mel-spec', fontsize=16)\n\nfor idx, samplename in enumerate(sample_names):\n    row, col = divmod(idx, 5)\n    ax = axes[row][col]\n    ax.imshow(all_bird_data[samplename][0],\n              origin='lower',\n              aspect='auto',\n              cmap='viridis')\n    ax.set_title(samplename, fontsize=8)\n    ax.axis('off')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:43:49.436628Z","iopub.execute_input":"2025-05-15T12:43:49.436922Z","iopub.status.idle":"2025-05-15T12:43:52.705407Z","shell.execute_reply.started":"2025-05-15T12:43:49.436898Z","shell.execute_reply":"2025-05-15T12:43:52.704194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the pickle files\nimport os, pickle\n\n# make sure /kaggle/working exists\nos.makedirs('/kaggle/working', exist_ok=True)\n\n# 1) Save the DataFrame (metadata)\ndf_path = '/kaggle/working/working_df.pkl'\nworking_df.to_pickle(df_path)\nprint(f\"✅ working_df saved to {df_path}\")\n\n# 2) Save the feature dict\ndata_path = '/kaggle/working/all_bird_data.pkl'\nwith open(data_path, 'wb') as f:\n    pickle.dump(all_bird_data, f)\nprint(f\"✅ all_bird_data saved to {data_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:43:52.706623Z","iopub.execute_input":"2025-05-15T12:43:52.707007Z","iopub.status.idle":"2025-05-15T12:43:52.794269Z","shell.execute_reply.started":"2025-05-15T12:43:52.706972Z","shell.execute_reply":"2025-05-15T12:43:52.793304Z"}},"outputs":[],"execution_count":null}]}